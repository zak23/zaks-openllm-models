"mistral:7b-capybara-fp16":
  project: vllm-chat
  service_config:
    name: capybarahermes2_5
    workers: 1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3090
  engine_config:
    model: argilla/CapybaraHermes-2.5-Mistral-7B
    max_model_len: 28704
    enforce_eager: true
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 7b
    model_name: argilla/CapybaraHermes-2.5-Mistral-7Bv0.1
  max_tokens:
    default: 28704
    maximum: 28704
    minimum: 128
    title: Max Tokens
    type: integer
"mistral:12b-dolphin-fp16":
  project: vllm-chat
  service_config:
    name: dolphin2_9_3
    workers: 2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-rtx-3090
  engine_config:
    model: cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b
    max_model_len: 20480
    enforce_eager: true
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 12b
    model_name: cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b
  max_tokens:
  default: 20480
  maximum: 20480
  minimum: 128
  title: Max Tokens
  type: integer
"mistral:7b-dolphin-bf16":
  project: vllm-chat
  service_config:
    name: dolphin2_9_3
    workers: 2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-rtx-3090
  engine_config:
    model: cognitivecomputations/dolphin-2.9.3-mistral-7B-32k
    max_model_len: 28704
    enforce_eager: true
    dtype: half
    tensor_parallel_size: 1
  extra_labels:
    openllm_alias: 7b
    model_name: cognitivecomputations/dolphin-2.9.3-mistral-7B-32k
  max_tokens:
  default: 28704
  maximum: 28704
  minimum: 128
  title: Max Tokens
  type: integer


